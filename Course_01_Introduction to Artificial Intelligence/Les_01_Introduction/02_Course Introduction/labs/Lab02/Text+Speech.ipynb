{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and Speech Analysis\n",
    "This notebook includes some basic examples of techniques used to analyze text and speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Basic Frequency Analysis\n",
    "Let's start by using some very basic frequency analysis on a document to see if we can determine what the document is about  based on word frequency.\n",
    "\n",
    "### Load a Text Document\n",
    "Run the cell below to load a document and view the text it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Curl to get a document from GitHub and open it\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Moon.txt -o Moon.txt\n",
    "doc1 = open(\"Moon.txt\", \"r\")\n",
    "\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Text\n",
    "Text includes a lot of punctuation, which we need to remove if we want to work only with the actual words.\n",
    "\n",
    "Run the cell below to strip all the punctuation from the text and convert the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# remove numeric digits\n",
    "txt = ''.join(c for c in doc1Txt if not c.isdigit())\n",
    "\n",
    "# remove punctuation and make lower case\n",
    "txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
    "\n",
    "# print the normalized text\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Frequency Distribution\n",
    "Now let's tokenize the text (split it into individual words), and count the number of times each word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "# Get the frequency distribution of the words into a data frame\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "print (count_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Word Frequency\n",
    "It's often easier to analyze frequency by creating a visualization, such as a Pareto chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords\n",
    "A large number of the words in the text are common words like \"the\" or \"and\". These \"stopwords\" add little in the way of semantic meaning to the text, and won't help us determine the subject matter - so run the cell below to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "print(\"\\n\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Frequency Distribution for the Remaining Words\n",
    "Now that we've prepared the text, we can tokenize the string into a list of individual words, and then perform frequency analysis on those words based on how often they appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency distribution of the remaining words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot the frequency of the top 60 words\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent word is \"new\", and other common words include \"go\", \"space\", \"science\", and \"moon\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Term Frequency - Inverse Document Frequency\n",
    "In the previous example, we've used basic term frequency to determine each word's \"importance\" based on how often it appears in the document. When dealing with a large corpus of multiple documents, a more commonly used technique is *term frequency, inverse document frequency* (or TF-IDF) in which a score is calculated based on how often a word or term appears in one document compared to its more general frequency across the entire collection of documents. Using this technique, a high degree of relevance is assumed for words that appear frequently in a particular document, but relatively infrequently across a wide range of other documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and View Two More Documents\n",
    "Let's add a couple more documents to our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind ourselves of the first document\n",
    "print(doc1Txt)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Get a second document, normalize it, and remove stop words\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Gettysburg.txt -o Gettysburg.txt\n",
    "doc2 = open(\"Gettysburg.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print (doc2Txt)\n",
    "from string import punctuation\n",
    "txt2 = ''.join(c for c in doc2Txt if not c.isdigit())\n",
    "txt2 = ''.join(c for c in txt2 if c not in punctuation).lower()\n",
    "txt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "\n",
    "# and a third\n",
    "print(\"------------------------------------------------\")\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Cognitive.txt -o Cognitive.txt\n",
    "doc3 = open(\"Cognitive.txt\", \"r\")\n",
    "doc3Txt = doc3.read()\n",
    "print (doc3Txt)\n",
    "from string import punctuation\n",
    "txt3 = ''.join(c for c in doc3Txt if not c.isdigit())\n",
    "txt3 = ''.join(c for c in txt3 if c not in punctuation).lower()\n",
    "txt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get TF-IDF Values for the top three words in each document\n",
    "Now we'll install the **textblob** library and create some functions that we'll use to find the top 3 most important words in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install textblob library and define functions for TF-IDF\n",
    "!pip install -U textblob\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    lenOfDoc = len(doc.words)\n",
    "    if lenOfDoc < 1: return 0\n",
    "    else: return doc.words.count(word) / lenOfDoc\n",
    "\n",
    "def contains(word, docs):\n",
    "    return sum(1 for doc in docs if word in doc.words)\n",
    "\n",
    "def idf(word, docs):\n",
    "    docsCount = contains(word, docs)\n",
    "    if docsCount < 1 : return 0\n",
    "    else: return math.log(len(docs) / docsCount)\n",
    "\n",
    "def tfidf(word, doc, docs):\n",
    "    return tf(word,doc) * idf(word, docs)\n",
    "\n",
    "\n",
    "# Create a collection of documents as textblobs\n",
    "doc1 = tb(txt)\n",
    "doc2 = tb(txt2)\n",
    "doc3 = tb(txt3)\n",
    "docs = [doc1, doc2, doc3]\n",
    "\n",
    "# Use TF-IDF to get the three most important words from each document\n",
    "print('-----------------------------------------------------------')\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Until now, we've simply counted the number of occurrances of each word. This doesn't take into account the fact that sometimes multiple words may be based on the same common base, or *stem*; and may be semantically equivalent. For example, \"fishes\", \"fished\", \"fishing\", and \"fisher\" are all derived from the stem \"fish\".\n",
    "\n",
    "### View frequency of words from Kennedy's inauguration speech\n",
    "Let's look at another document and count the unstemmed words it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and print text\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Inaugural.txt -o Inaugural.txt\n",
    "doc4 = open(\"Inaugural.txt\", \"r\", encoding=\"utf-16\")\n",
    "doc4Txt = doc4.read()\n",
    "\n",
    "print(doc4Txt)\n",
    "\n",
    "# Normalize and remove stop words\n",
    "from string import punctuation\n",
    "doc4Txt = ''.join(c for c in doc4Txt if not c.isdigit())\n",
    "doc4Txt = ''.join(c for c in doc4Txt if c not in punctuation).lower()\n",
    "doc4Txt = ' '.join([word for word in doc4Txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get Frequency distribution\n",
    "words = nltk.tokenize.word_tokenize(doc4Txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem the words using the Porter stemmer\n",
    "Now let's stem the words and count the stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Get the word stems\n",
    "ps = PorterStemmer()\n",
    "stems = [ps.stem(word) for word in words]\n",
    "\n",
    "# Get Frequency distribution\n",
    "fdist = FreqDist(stems)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the frequencies of the stems to the unstemmed words. Note in particular that the unstemmed woprds include \"power\", \"powers\", and \"powerful\", which are all stemmed to \"power\". Additionally, the text includes \"nations\" and \"nation\" - which also have a common stem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Text Analyics Cognitive Service\n",
    "The previous examples demonstrate some ways to write code and analyze text, and they serve to illustrate that text analytics involves applying statistical techniques to text data in order to discern semantic meaning. This is a common theme in many AI solutions.\n",
    "\n",
    "Microsoft Cognitive Services includes a Text Analytics service that encapsulates much more sophisticated techniques for ascertaining meaning from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Text Analytics Service\n",
    "To see this in action, you need to provision a Text Analytics service in your Azure subscription. Follow these steps to do that:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n",
    "4. In the list of cognitive services, click **Text Analytics API**.\n",
    "5. In the **Text Analytics API** blade, click **Create**.\n",
    "6. In the **Create** blade, enter the following details, and then click **Create**\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Subscription**: Your Azure subscription.\n",
    "  * **Location**: Choose the Azure datacenter location where you want to host your service.\n",
    "  * **Pricing tier**: Choose the F0 pricing tier.\n",
    "  * **Resource Group**: Choose the existing resource group you created in the previous lab (or create a new one if you didn't complete the previous lab)\n",
    "  * Read the notice about the use of your data, and select the checkbox.\n",
    "7. Wait for the service to be created.\n",
    "8. When deployment is complete, click **All Resources** and then click your Text Analytics service to open its blade.\n",
    "9. In the blade for your Text Analytics service, note the **Endpoint** URL. Then assign the base URI (*location*.api.cognitive.microsoft.com) for your service to the **textAnalyticsURI** variable in the cell below.\n",
    "10. In the blade for your Text Analytics service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **textKey** variable assignment value in the cell below. \n",
    "11. Run the cell below to assign the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textAnalyticsURI = 'eastus2.api.cognitive.microsoft.com'\n",
    "textKey = '123456789abcdefg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Text Analytics Service to Determine Key Phrases in the Documents\n",
    "One of the methods provided by the Text Analytics service is the ability to extract a list of key phrases from text documents, which give an insight into the core topics discussed in the document.\n",
    "\n",
    "Run the following cell to call the **keyPhrases** method of the Text Analytics service and extract the key phrases for the text documents you have loaded so far in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "\n",
    "# Define the request headers.\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': textKey,\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the parameters\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "# Define the request body\n",
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": doc1Txt\n",
    "    },\n",
    "    {\n",
    "          \"language\": \"en\",\n",
    "          \"id\": \"2\",\n",
    "          \"text\": doc2Txt\n",
    "    },\n",
    "    {\n",
    "          \"language\": \"en\",\n",
    "          \"id\": \"3\",\n",
    "          \"text\": doc3Txt\n",
    "    },\n",
    "    {\n",
    "          \"language\": \"en\",\n",
    "          \"id\": \"4\",\n",
    "          \"text\": doc4Txt\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute the REST API call and get the response.\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/keyPhrases?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "    # 'data' contains the JSON data. The following formats the JSON data for display.\n",
    "    parsed = json.loads(data)\n",
    "    for document in parsed['documents']:\n",
    "        print(\"Document \" + document[\"id\"] + \" key phrases:\")\n",
    "        for phrase in document['keyPhrases']:\n",
    "            print(\"  \" + phrase)\n",
    "        print(\"---------------------------\")\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these key phrases, it's reasonably clear that the first document is about freedom and nationhood, while the second is about software services for AI.\n",
    "\n",
    "### Perform Sentiment Analysis\n",
    "Another common AI requirement is to determine the sentiment associated with some text. For example, you might analyze tweets that include your organization's twitter handle to determine if they are positive or negative.\n",
    "\n",
    "Run the cell below to use the **sentiment** method of the Text Analytics service to discern the sentiment of two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Wow! cognitive services are fantastic.\"\n",
    "    },\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"2\",\n",
    "      \"text\": \"I hate it when computers don't understand me.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "    parsed = json.loads(data)\n",
    "    for document in parsed['documents']:\n",
    "        sentiment = \"negative\"\n",
    "        if document[\"score\"] >= 0.5:\n",
    "            sentiment = \"positive\"\n",
    "        print(\"Document:\" + document[\"id\"] + \" = \" + sentiment)\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Working with Speech\n",
    "So far, we've seen how analyze text; but increasingly AI systems enable humans to communicate with software services through speech recognition.\n",
    "\n",
    "### Create a Bing Speech Service\n",
    "The Microsoft Cognitive Services include the Bing Speech service, that can interpret spoken input from a microphone or audio file. Follow these steps to provision the Bing speech service:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n",
    "4. In the list of cognitive services, click **Bing Speech API**.\n",
    "5. In the **Bing Speech API** blade, click **Create**.\n",
    "6. In the **Create** blade, enter the following details, and then click **Create**\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Subscription**: Your Azure subscription.\n",
    "  * **Pricing tier**: Choose the F0 pricing tier.\n",
    "  * **Resource Group**: Choose the existing resource group you used previously.\n",
    "  * Read the notice about the use of your data, and select the checkbox.\n",
    "7. Wait for the service to be created.\n",
    "8. When deployment is complete, click **All Resources** and then click your Bing Speech service to open its blade.\n",
    "10. In the blade for your Bing Speech service, click **Keys** and then copy **Key 1** to the clipboard and paste it into the **textKey** variable assignment value in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechKey = '12345abcde'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the SpeechRecognition Package\n",
    "You can call the Bing Speech API by sending HTTP requests containing a JSON payload as you did for the Text Analytics API. However, the Bing Speech API is supported in a pre-built Python package named **SpeechRecognition**, which makes it easier to use. You can find out more about this package at https://pypi.python.org/pypi/SpeechRecognition.\n",
    "\n",
    "Run the following cell to install the **SpeechRecognition** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen to the Speech\n",
    "In this exercise, you will use a .wav audio file. To hear the speech you will analyze, run the cell below (this assumes you have audio capabilities in your computer!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/RainSpain.wav -o RainSpain.wav\n",
    "\n",
    "IPython.display.Audio('RainSpain.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Bing Speech Service to Transcribe the Audio\n",
    "Hopefully you understood what was said in the audio file.\n",
    "\n",
    "Let's see how the Bing Speech API does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Read the audio file\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile('RainSpain.wav') as source:\n",
    "    audio = r.record(source)\n",
    "    \n",
    "# Alternative code to use mic input (only works in local Jupyter - not in Azure Notebooks)\n",
    "# r = sr.Recognizer()\n",
    "# with sr.Microphone() as source:\n",
    "#     print(\"Say something!\")\n",
    "#     audio = r.listen(source)\n",
    "\n",
    "# transcribe speech using the Bing Speech API\n",
    "try:\n",
    "    transcription = r.recognize_bing(audio, key=speechKey)\n",
    "    print(\"Here's what I heard:\")\n",
    "    print('\"' + transcription + '\"')\n",
    "    \n",
    "except sr.UnknownValueError:\n",
    "    print(\"The audio was unclear\")\n",
    "except sr.RequestError as e:\n",
    "    print (e)\n",
    "    print(\"Something went wrong :-(; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Language Understanding Intelligence Service (LUIS)\n",
    "Increasingly, we expect computers to be able to use AI in order to understand spoken or typed commands in natural language. For example, we want to be able to say \"switch on the light\" or \"put the light on\", and have an AI-powered device understand the command and take appropriate action.\n",
    "\n",
    "### Provision the Language Understanding Intelligence Service (LUIS)\n",
    "The Microsoft cognitive services include the Language Understanding Intelligence Service (LUIS), which enables you to define *intents* that are applied to *entities* based on *utterances*.\n",
    "\n",
    "To get started with LUIS, follow these steps to provision the service in your Azure subscription:\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ New**, and in the **AI + Cognitive Services** category, click **See all**.\n",
    "4. In the list of cognitive services, click **Language Understanding**.\n",
    "5. In the **Language Understanding** blade, click **Create**.\n",
    "6. In the **Create** blade, enter the following details, and then click **Create**\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Subscription**: Your Azure subscription.\n",
    "  * **Location**: Choose a location in the US.\n",
    "  * **Pricing tier**: Choose the F0 pricing tier.\n",
    "  * **Resource Group**: Choose the existing resource group you used previously.\n",
    "  * Read the notice about the use of your data, and select the checkbox.\n",
    "7. Wait for the service to be created.\n",
    "\n",
    "### Create a LUIS App\n",
    "To implement natural language understanding with LUIS, you must first create an app; and then add intents, utterances, and entities to define the commands you want the app to understand.\n",
    "1. Open a new browser tab and navigate to https://www.luis.ai/.\n",
    "2. Sign in using the Microsoft account associated with your Azure subscription. If this is the first time you have signed into LUIS, you may need to grant the app some permissions to access your account details, and then fill in some information and accept the terms of use.\n",
    "3. If a message prompting you to complete a tutorial in which you will create a *Scheduler* app is displayed, close it (you can complete this tutorial later - for now, we'll focus on a simpler example).\n",
    "4. Click **New App** and create a new app with the following settings:\n",
    "  * **Name**: Simple Home Automation\n",
    "  * **Culture**: English\n",
    "  * **Description**: A basic home automation example\n",
    "5. In the pane on the left, click **Intents**. Then click **Add Intent**, and add an intent with the name **Light On**.\n",
    "6. In the **Utterances** page for the **Light On** intent, type \"*switch the light on*\" and press **Enter** to add this utterance to the list.\n",
    "7. In the list of utterances, in the *switch the light on* utterance, hold the mouse over the word \"light\" so that the list shows the value *switch the [light] on*. Then in the **Search or create** box type *Light* and create a simple entity named **Light**.\n",
    "8. In the pane on the left, click **Intents** and click **Add Intent**, to add a second intent with the name **Light Off**.\n",
    "9. In the **Utterances** page for the **Light Off** intent, type \"*switch the light off*\" and press **Enter** to add this utterance to the list.\n",
    "10. In the list of utterances, in the *switch the light on* utterance, hold the mouse over the word \"light\" so that the list shows the value *switch the [light] on*. Then click **[light]** select the *Light* entity you created previously.\n",
    "11. At the top of the page, click **Train** to train the application\n",
    "12. After the app has been trained, click **Test**, and then in the test pane, enter the following utterances and verify that they are correctly interpreted as commands for the *Light On* and *Light Off* intents respectively:\n",
    "    * *turn on the light*\n",
    "    * *put the light off*\n",
    "13. IAt the top of the page, click **Publish**. Then click **Publish to production slot**.\n",
    "14. After the app has been published, note the **Endpoint** URL that is generated for your app - this includes the location where you provisioned the service, your app ID, and the key assigned to the app.\n",
    "\n",
    "### Consume the LUIS App\n",
    "Now that you have published your LUIS app, you can consume it from a client application by making HTTP requests that include a query string. The query will be used to identify the most likely intent, which will be returned to the calling client as in JSON response.\n",
    "\n",
    "Modify the **endpointUrl** variable declaration in the cell below to reflect the endpoint URL for your app. Then run the cell, and enter a command when prompted to call your service and interpret the command. The JSON response is shown with an appropriate image for each command.\n",
    "\n",
    "Try the following commands:\n",
    "* *Switch on the light*\n",
    "* *Turn on the light*\n",
    "* *Turn off the light*\n",
    "* *Could you put the light on please?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json \n",
    "\n",
    "# Set up API configuration\n",
    "endpointUrl = \"https://westus.api.cognitive.microsoft.com/luis/v2.0/apps/12345abcde?subscription-key=12345abcde&verbose=true&timezoneOffset=0&q=\"\n",
    "\n",
    "# prompt for a command\n",
    "command = input('Please enter a command: \\n')\n",
    "\n",
    "# Call the LUIS service and get the JSON response\n",
    "endpoint = endpointUrl + command.replace(\" \",\"+\")\n",
    "response = requests.get(endpoint)\n",
    "data = json.loads(response.content.decode(\"UTF-8\"))\n",
    "print (data)\n",
    "\n",
    "# Identify the top scoring intent\n",
    "intent = data[\"topScoringIntent\"][\"intent\"]\n",
    "if (intent == \"Light On\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\n",
    "elif (intent == \"Light Off\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\n",
    "else:\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n",
    "\n",
    "# Get the appropriate image and show it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Speech Recognition and Natural Language Understanding\n",
    "An obvious next step is to combine speech recognition with natural language understanding so that a spoken command can be interpreted and the appropriate action taken.\n",
    "\n",
    "### Download Command Audio\n",
    "Let's start by downloading and playing some spoken commands for our home automation system. Run the two cells under this to hear the commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# Get the \"lights on\" command\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.wav -o LightOn.wav\n",
    "    \n",
    "IPython.display.Audio('LightOn.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the \"lights on\" command\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.wav -o LightOff.wav\n",
    "    \n",
    "IPython.display.Audio('LightOff.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe and Interpret the \"Light On\" Command\n",
    "Now you can use the Bing Speech API to transcribe the command, and then use your LUIS app to interpret the command and take the appropriate action.\n",
    "\n",
    "You should have already installed the **SpeechRecognition** package, set the **speechKey** for your Bing Speech service, and set the **apiUrl**, **appId**, and **appKey** for your LUIS app (if you have closed and re-opened the notebook, re-run the appropriate cells above to set these).\n",
    "\n",
    "Run the cell below that to test the \"Light On\" command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import speech_recognition as sr\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json \n",
    "\n",
    "# Specify which audio file to use\n",
    "audioFile = \"LightOn.wav\"\n",
    "\n",
    "# Read the audio file\n",
    "r = sr.Recognizer()\n",
    "with sr.AudioFile(audioFile) as source:\n",
    "    audio = r.record(source)\n",
    "\n",
    "try:\n",
    "    # transcribe speech using the Bing Speech API\n",
    "    transcription = r.recognize_bing(audio, key=speechKey)\n",
    "    \n",
    "    # Call the LUIS service and get the JSON response\n",
    "    endpoint = endpointUrl + transcription.replace(\" \",\"+\")\n",
    "    response = requests.get(endpoint)\n",
    "    data = json.loads(response.content.decode(\"UTF-8\"))\n",
    "\n",
    "    # Identify the top scoring intent\n",
    "    intent = data[\"topScoringIntent\"][\"intent\"]\n",
    "    if (intent == \"Light On\"):\n",
    "        img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\n",
    "    elif (intent == \"Light Off\"):\n",
    "        img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\n",
    "    else:\n",
    "        img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n",
    "\n",
    "    # Get the appropriate image and show it\n",
    "    response = requests.get(img_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    imshow(img)\n",
    "    \n",
    "except sr.UnknownValueError:\n",
    "    print(\"Bing Speech could not understand audio\")\n",
    "except sr.RequestError as e:\n",
    "    print (e)\n",
    "    print(\"Could not request results from the Bing Speech service; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe and Interpret the \"Light Off\" Command\n",
    "Modify the cell above to set the **audioFile** variable to the **LightOff.wav** audio file, and then run the cell again to test it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
