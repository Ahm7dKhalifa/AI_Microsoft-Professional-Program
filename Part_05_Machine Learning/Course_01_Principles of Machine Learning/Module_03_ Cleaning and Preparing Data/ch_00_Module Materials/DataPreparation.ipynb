{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "\n",
    "**Data preparation** is a vital step in the machine learning pipeline. Just as visualization is necessary to understand the relationships in data, proper preparation or **data munging** is required to ensure machine learning models work optimally. \n",
    "\n",
    "The process of data preparation is highly interactive and iterative. A typical process includes at least the following steps:\n",
    "1. **Visualization** of the dataset to understand the relationships and identify possible problems with the data.\n",
    "2. **Data cleaning and transformation** to address the problems identified. It many cases, step 1 is then repeated to verify that the cleaning and transformation had the desired effect. \n",
    "3. **Construction and evaluation of a machine learning models**. Visualization of the results will often lead to understanding of further data preparation that is required; going back to step 1. \n",
    "\n",
    "In this lab you will learn the following: \n",
    "- Recode character strings to eliminate characters that will not be processed correctly.\n",
    "- Find and treat missing values. \n",
    "- Set correct data type of each column. \n",
    "- Transform categorical features to create categories with more cases and coding likely to be useful in predicting the label. \n",
    "- Apply transformations to numeric features and the label to improve the distribution properties. \n",
    "- Locate and treat duplicate cases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example\n",
    "\n",
    "As a first example you will prepare the automotive dataset. Careful preparation of this dataset, or any dataset, is required before atempting to train any machine learning model. This dataset has a number of problems which must be addressed. Further, some feature engineering will be applied. \n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "As a first step you must load the dataset. \n",
    "\n",
    "Execute the code in the cell below to load the packages required  to run this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to load the dataset and print the first few rows of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices = pd.read_csv('Automobile price data _Raw_.csv')\n",
    "auto_prices.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now perform some data preparation steps. \n",
    "\n",
    "Recall the function clean_auto_data() in the previous lab. The next few sections perform the same tasks to the clean_auto_data() function, with detail explanations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode names\n",
    "\n",
    "Notice that several of the column names contain the '-' character. Python will not correctly recognize character strings containing '-'.  Rather, such a name will be recognized as two character strings. The same problem will occur with column values containing many special characters including, '-', ',', '*', '/', '|', '>', '<', '@', '!' etc. If such characters appear in column names of values, they must be replaced with another character. \n",
    "\n",
    "Execute the code in the cell below to replace the '-' characters by '_':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices.columns = [str.replace('-', '_') for str in auto_prices.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat missing values\n",
    "\n",
    "**Missing values** are a common problem in data set. Failure to deal with missing values before training a machine learning model will lead to biased training at best, and in many cases actual failure. The Python scikit-learn package will not process arrays with missing values. \n",
    "\n",
    "There are two problems that must be deal with when treating missing values:\n",
    "1. First you must find the missing values. This can be difficult as there is no standard way missing values are coded. Some common possibilities for missing values are:\n",
    "  - Coded by some particular character string, or numeric value like -999. \n",
    "  - A NULL value or numeric missing value such as a NaN. \n",
    "2. You must determine how to treat the missing values:\n",
    "  - Remove features with substantial numbers of missing values. In many cases, such features are likely to have little information value. \n",
    "  - Remove rows with missing values. If there are only a few rows with missing values it might be easier and more certain to simply remove them. \n",
    "  - Impute values. Imputation can be done with simple algorithms such as replacing the missing values with the mean or median value. There are also complex statistical methods such as the expectation maximization (EM) or SMOTE algorithms. \n",
    "  - Use nearest neighbor values. Alternatives for nearest neighbor values include, averaging, forward filling or backward filling. \n",
    "  \n",
    "Carefully observe the first few cases from the data frame and notice that missing values are coded with a '?' character. Execute the code in the cell below to identify the columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(auto_prices.astype(np.object) == '?').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to display the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the columns with missing values to their data types. In all cases, the columns with missing values have an `object` (character) type as a result of using the '?' code. As a result, some columns that should be numeric (bore, stroke, horsepower, peak_rpm, and price) are coded as `object`.\n",
    "\n",
    "The next question is how many missing values are in each of these `object` type columns? Execute the code in the cell below to display the counts of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in auto_prices.columns:\n",
    "    if auto_prices[col].dtype == object:\n",
    "        count = 0\n",
    "        count = [count + 1 for x in auto_prices[col] if x == '?']\n",
    "        print(col + ' ' + str(sum(count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `normalize_losses` column has a significant number of missing values and will be removed. Columns that should be numeric, but contain missing values, are processed in the following manner:\n",
    "1. The '?' values are computed to Pandas Numpy missing values `nan`.\n",
    "2. Rows containing `nan` values are removed. \n",
    "\n",
    "Execute this code, noticing the resulting shape of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop column with too many missing values\n",
    "auto_prices.drop('normalized_losses', axis = 1, inplace = True)\n",
    "## Remove rows with missing values, accounting for mising values coded as '?'\n",
    "cols = ['price', 'bore', 'stroke',\n",
    "          'horsepower', 'peak_rpm']\n",
    "for column in cols:\n",
    "    auto_prices.loc[auto_prices[column] == '?', column] = np.nan\n",
    "auto_prices.dropna(axis = 0, inplace = True)\n",
    "auto_prices.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set now contains 195 cases and 25 columns. 10 rows have been dropped by removing missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform column data type\n",
    "\n",
    "As has been previously noted, there are five columns in this dataset which do not have the correct type as a result of missing values. This is a common situation, as the methods used to automatically determine data type when loading files can fail when missing values are present. \n",
    "\n",
    "The code in the cell below iterates over a list of columns setting them to numeric. Execute this code and observe the resulting  types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in cols:\n",
    "    auto_prices[column] = pd.to_numeric(auto_prices[column])\n",
    "auto_prices[cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and transforming variables\n",
    "\n",
    "In most cases, machine learning is not done with the raw features. Features are transformed, or combined to form new features in forms which are more predictive. This process is known as **feature engineering**. In many cases, good feature engineering is more important than the details of the machine learning model used. It is often the case that good features can make even poor machine learning models work well, whereas, given poor features even the best machine learning model will produce poor results. As the famous saying goes, \"garbage in, garbage out\".\n",
    "\n",
    "Some common approaches to feature engineering include:\n",
    "- **Aggregating categories** of categorical variables to reduce the number. Categorical features or labels with too many unique categories will limit the predictive power of a machine learning model. Aggregating categories can improve this situation, sometime greatly. However, one must be careful. It only makes sense to aggregate categories that are similar in the domain of the problem. Thus, domain expertise must be applied. \n",
    "- **Transforming numeric variables** to improve their distribution properties to make them more covariate with other variables. This process can be applied not only features, but to labels for regression problems. Some common transformations include, **logarithmic** and **power** included squares and square roots. \n",
    "- **Compute new features** from two or more existing features. These new features are often referred to as **interaction terms**. An interaction occurs when the behavior of say, the produce of the values of two features, is significantly more predictive than the two features by themselves. Consider the probability of purchase for a luxury mens' shoe. This probability depends on the interaction of the user being a man and the buyer being wealthy. As another example, consider the number of expected riders on a bus route. This value will depend on the interaction between the time of day and if it is a holiday. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregating categorical variables\n",
    "\n",
    "When a dataset contains categorical variables these need to be investigated to ensure that each category has sufficient samples. It is commonly the case that some categories may have very few samples, or have so many similar categories as to be meaningless. \n",
    "\n",
    "As a specific case, you will examine the number of cylinders in the cars. Execute the code in the cell below to print a frequency table for this variable and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices['num_of_cylinders'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is only one car with three and twelve cylinders. There are only four cars with eight cylinders, and 10 cars with five cylinders. It is likely that all of these categories will not have statistically significant difference in predicting auto price. It is clear that these categories need to be aggregated. \n",
    "\n",
    "The code in the cell below uses a Python dictionary to recode the number of cylinder categories into a smaller number categories. Execute this code and examine the resulting frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cylinder_categories = {'three':'three_four', 'four':'three_four', \n",
    "                    'five':'five_six', 'six':'five_six',\n",
    "                    'eight':'eight_twelve', 'twelve':'eight_twelve'}\n",
    "auto_prices['num_of_cylinders'] = [cylinder_categories[x] for x in auto_prices['num_of_cylinders']]\n",
    "auto_prices['num_of_cylinders'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now three categories. One of these categories only has five members. However, it is likely that these autos will have different pricing from others.\n",
    "\n",
    "Next, execute the code in the cell below to make box plots of the new cylinder categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(auto_prices, col, col_y = 'price'):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.boxplot(col, col_y, data=auto_prices)\n",
    "    plt.xlabel(col) # Set text for the x axis\n",
    "    plt.ylabel(col_y)# Set text for y axis\n",
    "    plt.show()\n",
    "    \n",
    "plot_box(auto_prices, 'num_of_cylinders')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the price range of these categories is distinctive. It is likely that these new categories will be useful in predicting the price of autos. \n",
    "\n",
    "Now, execute the code in the cell below and examine the frequency table for the `body_style` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices['body_style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of these categories have a limited number of cases. These categories can be aggregated to increase the number of cases using a similar approach as used for the number of cylinders. Execute the code in the cell below to aggregate these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_cats = {'sedan':'sedan', 'hatchback':'hatchback', 'wagon':'wagon', \n",
    "             'hardtop':'hardtop_convert', 'convertible':'hardtop_convert'}\n",
    "auto_prices['body_style'] = [body_cats[x] for x in auto_prices['body_style']]\n",
    "auto_prices['body_style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate if this aggregation of categories was a good idea, execute the code in the cell below to display a box plot. \n",
    "\n",
    "Then, answer **Question 1** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box(auto_prices, col, col_y = 'price'):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.boxplot(col, col_y, data=auto_prices)\n",
    "    plt.xlabel(col) # Set text for the x axis\n",
    "    plt.ylabel(col_y)# Set text for y axis\n",
    "    plt.show()\n",
    "    \n",
    "plot_box(auto_prices, 'body_style')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hardtop_convert` category does appear to have values distinct from the other body style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming numeric variables\n",
    "\n",
    "To improve performance of machine learning models transformations of the values are often applied. Typically, transformations are used to make the relationships between variables more linear. In other cases, transformations are performed to make distributions closer to Normal, or at least more symmetric. These transformations can include taking logarithms, exponential transformations and power transformations. \n",
    "\n",
    "In this case, you will transform the label, the price of the car. Execute the code in the cell below to display and examine a histogram of the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(vals, lab):\n",
    "    ## Distribution plot of values\n",
    "    sns.distplot(vals)\n",
    "    plt.title('Histogram of ' + lab)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    \n",
    "#labels = np.array(auto_prices['price'])\n",
    "hist_plot(auto_prices['price'], 'prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of auto price is both quite skewed to the left and multimodal. Given the skew and the fact that there are no values less than or equal to zero, a log transformation might be appropriate.\n",
    "\n",
    "The code in the cell below displays a histogram of the logarithm of prices. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_prices['log_price'] = np.log(auto_prices['price'])\n",
    "hist_plot(auto_prices['log_price'], 'log prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the logarithm of price is more symmetric, but still shows some multimodal tendency and skew. Nonetheless, this is an improvement so we will use these values as our label.\n",
    "\n",
    "The next question is, how does this transformation change the relationship between the label and some of the features? To find out, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_shape(auto_prices, cols, shape_col = 'fuel_type', col_y = 'log_price', alpha = 0.2):\n",
    "    shapes = ['+', 'o', 's', 'x', '^'] # pick distinctive shapes\n",
    "    unique_cats = auto_prices[shape_col].unique()\n",
    "    for col in cols: # loop over the columns to plot\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        for i, cat in enumerate(unique_cats): # loop over the unique categories\n",
    "            temp = auto_prices[auto_prices[shape_col] == cat]\n",
    "            sns.regplot(col, col_y, data=temp, marker = shapes[i], label = cat,\n",
    "                        scatter_kws={\"alpha\":alpha}, fit_reg = False, color = 'blue')\n",
    "        plt.title('Scatter plot of ' + col_y + ' vs. ' + col) # Give the plot a main title\n",
    "        plt.xlabel(col) # Set text for the x axis\n",
    "        plt.ylabel(col_y)# Set text for y axis\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "num_cols = ['curb_weight', 'engine_size', 'horsepower', 'city_mpg']\n",
    "plot_scatter_shape(auto_prices, num_cols)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results to those obtained in the visualization lab, it does appear that the relationships between curb_weight and log_price and city_mpg and log_price are more linear, compared to the relationships between curb_weight and price and city_mpg and price respectively.\n",
    "\n",
    "The relationship with the log_price and categorical variables should likely also be investigated. It is also possible that some type of power transformation should be applied to, say horsepower or engine_size. In the interest of brevity, these ideas are not pursued here. \n",
    "\n",
    "Before proceeding, answer **Question 2** on the course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe to a csv file \n",
    "# We will use this in the next module so that we don't have to re-do the steps above\n",
    "# You don't have to run this code as the csv file has been saved under the next module's folder\n",
    "#auto_prices.to_csv('Auto_Data_Preped.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example\n",
    "\n",
    "Next, you will prepare the German credit data. Execute the code in the cell below  to load the dataset and print the head (first 5 rows) of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('German_Credit.csv', header=None)\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a bit hard to understand. For a start, the column names are not human readable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode character strings \n",
    "\n",
    "You have likely noticed that the the column names are  not human readable. This can be changed as was done for the previous dataset. Execute the code in the cell below to add human-readable column names to the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.columns = ['customer_id', 'checking_account_status', 'loan_duration_mo', 'credit_history', \n",
    "                  'purpose', 'loan_amount', 'savings_account_balance', \n",
    "                  'time_employed_yrs', 'payment_pcnt_income','gender_status', \n",
    "                  'other_signators', 'time_in_residence', 'property', 'age_yrs',\n",
    "                  'other_credit_outstanding', 'home_ownership', 'number_loans', \n",
    "                  'job_category', 'dependents', 'telephone', 'foreign_worker', \n",
    "                  'bad_credit']\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, there is a trickier problem to deal with. The current coding of the categorical variables is impossible to understand. This makes interpreting these variables nearly impossible.  \n",
    "\n",
    "The code in the cell below uses a list of dictionaries to recode the categorical features with human-readable text. The final dictionary in the list recodes good and bad credit as a binary variable, $\\{ 0,1 \\}$. Two iterators are used to apply the dictionary:\n",
    "1. The `for` loop iterates over the columns and indexes the dictionary for the column. \n",
    "2. A list comprehension iterates of the values in the column and uses the dictionary to map the codes to human-readable category names. \n",
    "\n",
    "Execute this code and examine the result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_list = [['checking_account_status', \n",
    "              {'A11' : '< 0 DM', \n",
    "               'A12' : '0 - 200 DM', \n",
    "               'A13' : '> 200 DM or salary assignment', \n",
    "               'A14' : 'none'}],\n",
    "            ['credit_history',\n",
    "            {'A30' : 'no credit - paid', \n",
    "             'A31' : 'all loans at bank paid', \n",
    "             'A32' : 'current loans paid', \n",
    "             'A33' : 'past payment delays', \n",
    "             'A34' : 'critical account - other non-bank loans'}],\n",
    "            ['purpose',\n",
    "            {'A40' : 'car (new)', \n",
    "             'A41' : 'car (used)',\n",
    "             'A42' : 'furniture/equipment',\n",
    "             'A43' : 'radio/television', \n",
    "             'A44' : 'domestic appliances', \n",
    "             'A45' : 'repairs', \n",
    "             'A46' : 'education', \n",
    "             'A47' : 'vacation',\n",
    "             'A48' : 'retraining',\n",
    "             'A49' : 'business', \n",
    "             'A410' : 'other' }],\n",
    "            ['savings_account_balance',\n",
    "            {'A61' : '< 100 DM', \n",
    "             'A62' : '100 - 500 DM', \n",
    "             'A63' : '500 - 1000 DM', \n",
    "             'A64' : '>= 1000 DM',\n",
    "             'A65' : 'unknown/none' }],\n",
    "            ['time_employed_yrs',\n",
    "            {'A71' : 'unemployed',\n",
    "             'A72' : '< 1 year', \n",
    "             'A73' : '1 - 4 years', \n",
    "             'A74' : '4 - 7 years', \n",
    "             'A75' : '>= 7 years'}],\n",
    "            ['gender_status',\n",
    "            {'A91' : 'male-divorced/separated', \n",
    "             'A92' : 'female-divorced/separated/married',\n",
    "             'A93' : 'male-single', \n",
    "             'A94' : 'male-married/widowed', \n",
    "             'A95' : 'female-single'}],\n",
    "            ['other_signators',\n",
    "            {'A101' : 'none', \n",
    "             'A102' : 'co-applicant', \n",
    "             'A103' : 'guarantor'}],\n",
    "            ['property',\n",
    "            {'A121' : 'real estate',\n",
    "             'A122' : 'building society savings/life insurance', \n",
    "             'A123' : 'car or other',\n",
    "             'A124' : 'unknown-none' }],\n",
    "            ['other_credit_outstanding',\n",
    "            {'A141' : 'bank', \n",
    "             'A142' : 'stores', \n",
    "             'A143' : 'none'}],\n",
    "             ['home_ownership',\n",
    "            {'A151' : 'rent', \n",
    "             'A152' : 'own', \n",
    "             'A153' : 'for free'}],\n",
    "            ['job_category',\n",
    "            {'A171' : 'unemployed-unskilled-non-resident', \n",
    "             'A172' : 'unskilled-resident', \n",
    "             'A173' : 'skilled',\n",
    "             'A174' : 'highly skilled'}],\n",
    "            ['telephone', \n",
    "            {'A191' : 'none', \n",
    "             'A192' : 'yes'}],\n",
    "            ['foreign_worker',\n",
    "            {'A201' : 'yes', \n",
    "             'A202' : 'no'}],\n",
    "            ['bad_credit',\n",
    "            {2 : 1,\n",
    "             1 : 0}]]\n",
    "\n",
    "for col_dic in code_list:\n",
    "    col = col_dic[0]\n",
    "    dic = col_dic[1]\n",
    "    credit[col] = [dic[x] for x in credit[col]]\n",
    "    \n",
    "credit.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical values are now coded in a human readable manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicate rows\n",
    "\n",
    "Duplicate cases can seriously bias the training of machine learning models. In simple terms, cases which are duplicates add undue weight to that case when training a machine learning model. Therefore, it is necessary to ensure there are no duplicates in the dataset before training a model.  \n",
    "\n",
    "One must be careful when determining if a case is a duplicate or not. It is possible that some cases have identical values, particularly if most or all features are categorical. On the other hand, if there are columns with values guaranteed to be unique these can be used to detect and remove duplicates.\n",
    "\n",
    "Another consideration when removing duplicate cases is determining which case to remove. If the duplicates have different dates of creation, the newest date is often selected. In the absence of such a criteria, the choice is often arbitrary. You may chose to keep the first case or the last case. \n",
    "\n",
    "The German credit data has a customer_id column which should be unique. In the previous lab, we simply remove the customer_id. Turns out, this identifier column will be useful to determine duplicate rows. The presence of duplicates can be determined by comparing the number of rows to the number of unique values of the identifier column, in this case the customer_id column. The code in the cell below prints the shape of the data frame and the number of unique customer_id values. \n",
    "\n",
    "Execute this code, examine the results, and answer **Question 3** on the course page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit.shape)\n",
    "print(credit.customer_id.unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12 duplicate cases. These need to be located and the duplicates removed. In this case, the first instance will be kept. \n",
    "\n",
    "The code in the cell below removes these duplicates from the data frame inplace and the number of remaining rows and unique customer_ids are printed. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit.drop_duplicates(subset = 'customer_id', keep = 'first', inplace = True)\n",
    "print(credit.shape)\n",
    "print(credit.customer_id.unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicate rows have been successfully removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe to a csv file \n",
    "# We will use this in the next module so that we don't have to re-do the steps above\n",
    "# You don't have to run this code as the csv file has been saved under the next module's folder\n",
    "#credit.to_csv('German_Credit_Preped.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Some feature engineering needs to be investigated to determine if any improvement in predictive power can be expected. From the previous data exploration, it is apparent that several of the numeric features had a strong left skew. A log transformation may help in a case like this. \n",
    "\n",
    "Execute the code in the cell below uses the Pandas `applymap` method to apply the `log` function to each element of several columns in the data frame. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit[['log_loan_duration_mo', 'log_loan_amount', 'log_age_yrs']] = credit[['loan_duration_mo', 'loan_amount', 'age_yrs']].applymap(math.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to visualize the differences in the distributions of the untransformed and transformed variables for the two label values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = ['log_loan_duration_mo', 'log_loan_amount', 'log_age_yrs',\n",
    "                   'loan_duration_mo', 'loan_amount', 'age_yrs']\n",
    "\n",
    "for col in num_cols:\n",
    "    print(col)\n",
    "    _ = plt.figure(figsize = (10,4))\n",
    "    sns.violinplot(x= 'bad_credit', y = col, hue = 'bad_credit', \n",
    "                   data = credit)\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log transformed features have more symmetric distributions. However, it does not appear that the separation of the label cases is improved. These features will not be used further.\n",
    "\n",
    "****\n",
    "**Note:** Recalling the visualization of the categorical features, there are quite a few categories with few cases. However, it is not clear how these categories can be reasonably combined. It may be the case that some of these categorical features are not terribly predictive.\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Good data preparation is the key to good machine learning performance. Data preparation or data munging is a time interactive and iterative process. Continue to visualize the results as you test ideas. Expect to try many approaches, reject the ones that do not help, and keep the ones that do. In summary, test a lot of ideas, fail fast, keep what works. The reward is that well prepared data can improve the performance of almost any machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
